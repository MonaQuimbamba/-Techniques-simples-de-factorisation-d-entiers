{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonaQuimbamba/-Techniques-simples-de-factorisation-d-entiers/blob/master/chathunter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Scapy lib"
      ],
      "metadata": {
        "id": "weKHWalzUoG1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4xpuxwKHOGL"
      },
      "source": [
        "#install lib here\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "# Load Model here\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_trf\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports\n",
        "import json\n",
        "import spacy\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Image\n",
        "import requests\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "W3eiJe1EWIN9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping Code"
      ],
      "metadata": {
        "id": "HtpjTVYdR02u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Send an HTTP GET request to the webpage\n",
        "url = 'https://medium.com/@mohameddaher/from-p5-to-p5-to-p2-from-nothing-to-1000-bxss-4dd26bc30a82'\n",
        "url='https://medium.com/@TnMch/google-acquisition-xss-apigee-5479d7b5dc4'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "elements = soup.find_all(recursive=True)\n",
        "for element in elements:\n",
        "  if element.name not in ('script', 'input','footer','meta','link','head','style','a','html','title'):\n",
        "    if element.name == 'img':\n",
        "            img_url = element.get('src')\n",
        "            if img_url:\n",
        "              response = requests.get(img_url)\n",
        "              if response.status_code == 200:\n",
        "                  img = Image.open(BytesIO(response.content))\n",
        "                  display(img)\n",
        "    if element.get_text():\n",
        "      print(element.get_text())\n"
      ],
      "metadata": {
        "id": "nlunak0tR3uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store the Data\n",
        "\n"
      ],
      "metadata": {
        "id": "YDemfsN4SRG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# Save data to a CSV file\n",
        "with open('dataset.csv', 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['Column1', 'Column2'])  # Write header row\n",
        "    for item in data:\n",
        "        writer.writerow([item])"
      ],
      "metadata": {
        "id": "1ZPHWfFKR-8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "YQa46KYCSbtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean():\n",
        "  return \"\""
      ],
      "metadata": {
        "id": "YEfjalcYSeTp"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze and Extract the Data to build the **Dataset**\n",
        "\n"
      ],
      "metadata": {
        "id": "WEUBEh-aSouZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4cbLxCppSAFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def searchVuln():\n",
        "  \"\"\"\n",
        "   f that will get all vuln that we want search\n",
        "  \"\"\"\n",
        "  with open(\"Resources/vuln.json\", \"r\") as file:\n",
        "          #print(json.loads(data, strict=False))\n",
        "          return json.load(file,strict=False)"
      ],
      "metadata": {
        "id": "ter7BYpLhXbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def open_file(file_path):\n",
        "    \"\"\"Open a file and return its content as a string\"\"\"\n",
        "    try:\n",
        "        # Open the file in read mode ('r')\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            # Read the contents of the file\n",
        "            file_contents = file.read()\n",
        "            return file_contents.replace('\\n', ' ')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The file '{file_path}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cnYaEdzGXJC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the larger spaCy model\n",
        "nlp = spacy.load('en_core_web_lg')  # Use 'en_core_web_lg' for the largest model\n",
        "\n",
        "# Define a function to search for information\n",
        "def search_sentence_on_text(text, query):\n",
        "  \"\"\"Searches for information in a text using NLP.\n",
        "  Args:\n",
        "    text: The text to search.\n",
        "    query: The query to search for.\n",
        "\n",
        "  Returns:\n",
        "    A list of sentences that contain the query.\n",
        "  \"\"\"\n",
        "  # Preprocess the text and query\n",
        "  doc = nlp(text)\n",
        "  # Find all sentences in the text\n",
        "  sentences = list(doc.sents)\n",
        "  # Use spaCy's similarity score to rank sentences\n",
        "  ranked_sentences = []\n",
        "  for sentence in sentences:\n",
        "    similarity_score = nlp(query).similarity(sentence)\n",
        "    ranked_sentences.append((sentence, similarity_score))\n",
        "  # Filter sentences with similarity score above a threshold\n",
        "  results = [sentence.text for sentence, score in ranked_sentences if score > 0.7]\n",
        "  return results\n",
        "\n",
        "def search_word_on_text(text, target_word):\n",
        "    \"\"\"Searches for sentences that contain a target word in a text using NLP.\n",
        "\n",
        "    Args:\n",
        "        text: The text to search.\n",
        "        target_word: The word to search for in the sentences.\n",
        "\n",
        "    Returns:\n",
        "        A list of sentences that contain the target word.\n",
        "    \"\"\"\n",
        "    # Preprocess the text\n",
        "    doc = nlp(text)\n",
        "    # Find all sentences in the text\n",
        "    sentences = list(doc.sents)\n",
        "    # Filter sentences that contain the target word\n",
        "    results = [sentence.text for sentence in sentences if target_word in sentence.text]\n",
        "    return results\n",
        "\n",
        "\n",
        "def search_info(text, target_word):\n",
        "    \"\"\"Searches for words surrounding a target word in a text using NLP.\n",
        "\n",
        "    Args:\n",
        "        text: The text to search.\n",
        "        target_word: The word to search for in the sentences.\n",
        "\n",
        "    Returns:\n",
        "        A list of four-word phrases (2 words before and 2 words after) that contain the target word.\n",
        "    \"\"\"\n",
        "    # Preprocess the text\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Find all tokens in the text\n",
        "    tokens = doc.text.split()\n",
        "\n",
        "    # Initialize the results list\n",
        "    results = \"\"\n",
        "\n",
        "    # Iterate through tokens to find the target word\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token == target_word:\n",
        "            # Extract the surrounding words (2 words before and 3 words after)\n",
        "            start_idx = max(i - 1, 0)\n",
        "            end_idx = min(i + 2, len(tokens))\n",
        "            phrase = \" \".join(tokens[start_idx:end_idx])\n",
        "            results+=phrase\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "ThxkFoeSVKpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get text from webpage :\n",
        "file_path = \"Collection/website_content.txt\"\n",
        "text = open_file(file_path)\n",
        "\n",
        "### what we want get from the text\n",
        "\n",
        "#text = \"\"\"The current president of the United States is Joseph R. Biden, Jr. He was sworn into office on January 20, 2021.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "## search vuln on text\n",
        "vulnerabilities = searchVuln()\n",
        "for category, vulns in vulnerabilities.items():\n",
        "    for query in vulns:\n",
        "        results = search_sentence_on_text(text, query)\n",
        "        if not results:\n",
        "          break\n",
        "\n",
        "print(\"Vuln found : \",results)"
      ],
      "metadata": {
        "id": "9w_1_DxXRZpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XhnqLYV4hh84"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}